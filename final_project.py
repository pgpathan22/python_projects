# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k-hlpRUmyJ_dLGc3ioBsNS0BwrxObCjp
"""

import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load the dataset
df = pd.read_csv('/content/sample_data/ds_salaries.csv')

# Number of instances
num_instances = len(df)

# Number of features
num_features = df.shape[1]

# Number of instances from each class
class_counts = df['experience_level'].value_counts()

print("Number of instances:", num_instances)
print("Number of features:", num_features)
print("Number of instances from each class:")
print(class_counts)

#Load and split dataset
from sklearn.model_selection import train_test_split

# Splitting features (X) and target variable (y)
X = df.drop(columns=['salary', 'salary_currency', 'salary_in_usd'])
y = df['salary_in_usd']

# Splitting the dataset into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Checking the shape of each set
print("Training set shape:", X_train.shape)
print("Validation set shape:", X_val.shape)
print("Test set shape:", X_test.shape)

#EDA (Exploratory Data Analysis)
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = df

# Display basic information about the dataset
print("Dataset Info:")
print(data.info())

# Display the first few rows of the dataset
print("\nFirst few rows of the dataset:")
print(data.head())

# Summary statistics for numeric columns
print("\nSummary Statistics for Numeric Columns:")
print(data.describe())

# Check for missing values
print("\nMissing Values:")
print(data.isnull().sum())

# Check unique values in categorical columns
print("\nUnique Values in Categorical Columns:")
for col in data.select_dtypes(include=['object']).columns:
    print(f"{col}: {data[col].unique()}")

# Plotting

# Distribution of numeric features
numeric_features = ['salary', 'salary_in_usd', 'remote_ratio']
plt.figure(figsize=(12, 6))
for i, feature in enumerate(numeric_features, 1):
    plt.subplot(1, len(numeric_features), i)
    sns.histplot(data[feature], kde=True)
    plt.title(f"Distribution of {feature}")
plt.tight_layout()
plt.show()

# Boxplot for salary
plt.figure(figsize=(8, 6))
sns.boxplot(data=data[['salary', 'salary_in_usd']])
plt.title("Boxplot of Salary")
plt.ylabel("Salary (USD)")
plt.show()

# Countplot for categorical features
categorical_features = ['work_year', 'experience_level', 'employment_type', 'job_title',
                        'salary_currency', 'employee_residence', 'company_location', 'company_size']
plt.figure(figsize=(16, 14))
for i, feature in enumerate(categorical_features, 1):
    plt.subplot(4, 2, i)
    sns.countplot(data=data, x=feature)
    plt.title(f"Countplot of {feature}")
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
#End EDA

# Train the three classifiers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# Data Preparation
X = data.drop(['employment_type', 'job_title', 'salary_currency'], axis=1)
y = data['employment_type']

# Encoding categorical variables
label_encoders = {col: LabelEncoder().fit(X[col]) for col in X.select_dtypes(include=['object']).columns}
X = X.apply(lambda x: label_encoders[x.name].transform(x) if x.name in label_encoders else x)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression Classifier
clf_lr = make_pipeline(StandardScaler(), LogisticRegression())
clf_lr.fit(X_train, y_train)

y_pred_lr = clf_lr.predict(X_test)

# Classification Evaluation
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Classifier Accuracy:", accuracy_lr)
print("Classification Report for Logistic Regression Classifier:")
print(classification_report(y_test, y_pred_lr))

# Convert y_test and y_pred_lr to numerical labels for regression evaluation
le = LabelEncoder()
y_test_encoded = le.fit_transform(y_test)
y_pred_lr_encoded = le.transform(y_pred_lr)

# Regression Evaluation Metrics
mse = mean_squared_error(y_test_encoded, y_pred_lr_encoded)
mae = mean_absolute_error(y_test_encoded, y_pred_lr_encoded)
r2 = r2_score(y_test_encoded, y_pred_lr_encoded)

# Printing the evaluation results
print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("R-squared (R2) Score:", r2)

from sklearn.ensemble import RandomForestClassifier
import warnings
import sklearn.exceptions
warnings.filterwarnings("ignore", category=sklearn.exceptions.UndefinedMetricWarning)

# Random Forest Classifier
clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clf_rf.fit(X_train, y_train)
y_pred_rf = clf_rf.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Random Forest Classifier Accuracy:", accuracy_rf)
print("Classification Report for Random Forest Classifier:")
print(classification_report(y_test, y_pred_rf))

from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting Classifier
clf_gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
clf_gb.fit(X_train, y_train)
y_pred_gb = clf_gb.predict(X_test)

# Evaluation
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print("Gradient Boosting Classifier Accuracy:", accuracy_gb)
print("Classification Report for Gradient Boosting Classifier:")
print(classification_report(y_test, y_pred_gb))

# Plotting confusion matrix for Logistic Regression
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

cm_lr = confusion_matrix(y_test, y_pred_lr)

plt.figure(figsize=(10, 8))
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - Logistic Regression Classifier')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Plotting confusion matrix for Random Forest Classifier
cm_rf = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(10, 8))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - Random Forest Classifier')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Plotting confusion matrix for Gradient Boosting Classifier
cm_gb = confusion_matrix(y_test, y_pred_gb)

plt.figure(figsize=(10, 8))
sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - Gradient Boosting Classifier')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Discussion of the results
print("Discussion of the Results:")
print("The three classifiers, Logistic Regression, Random Forest, and Gradient Boosting, were trained and tested on the dataset.")
print("Their accuracies and classification reports were generated, providing insights into their performance.")
print("Additionally, confusion matrices were plotted to visualize the performance of each classifier in predicting different classes.")

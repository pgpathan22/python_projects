# -*- coding: utf-8 -*-
"""AssociationRuleMining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lfEH5lQxFmL0eRGq8jOWIyNt5Ne-xIVp
"""

# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k-hlpRUmyJ_dLGc3ioBsNS0BwrxObCjp
"""

import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load the dataset
df = pd.read_csv('/content/sample_data/ds_salaries.csv')

# Number of instances
num_instances = len(df)

# Number of features
num_features = df.shape[1]

# Number of instances from each class
class_counts = df['experience_level'].value_counts()

print("Number of instances:", num_instances)
print("Number of features:", num_features)
print("Number of instances from each class:")
print(class_counts)

from sklearn.model_selection import train_test_split

# Splitting features (X) and target variable (y)
X = df.drop(columns=['salary', 'salary_currency', 'salary_in_usd'])
y = df['salary_in_usd']

# Splitting the dataset into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Checking the shape of each set
print("Training set shape:", X_train.shape)
print("Validation set shape:", X_val.shape)
print("Test set shape:", X_test.shape)

#EDA
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = df

# Display basic information about the dataset
print("Dataset Info:")
print(data.info())

# Display the first few rows of the dataset
print("\nFirst few rows of the dataset:")
print(data.head())

# Summary statistics for numeric columns
print("\nSummary Statistics for Numeric Columns:")
print(data.describe())

# Check for missing values
print("\nMissing Values:")
print(data.isnull().sum())

# Check unique values in categorical columns
print("\nUnique Values in Categorical Columns:")
for col in data.select_dtypes(include=['object']).columns:
    print(f"{col}: {data[col].unique()}")

# Plotting

# Distribution of numeric features
numeric_features = ['salary', 'salary_in_usd', 'remote_ratio']
plt.figure(figsize=(12, 6))
for i, feature in enumerate(numeric_features, 1):
    plt.subplot(1, len(numeric_features), i)
    sns.histplot(data[feature], kde=True)
    plt.title(f"Distribution of {feature}")
plt.tight_layout()
plt.show()

# Boxplot for salary
plt.figure(figsize=(8, 6))
sns.boxplot(data=data[['salary', 'salary_in_usd']])
plt.title("Boxplot of Salary")
plt.ylabel("Salary (USD)")
plt.show()

# Countplot for categorical features
categorical_features = ['work_year', 'experience_level', 'employment_type', 'job_title',
                        'salary_currency', 'employee_residence', 'company_location', 'company_size']
plt.figure(figsize=(16, 14))
for i, feature in enumerate(categorical_features, 1):
    plt.subplot(4, 2, i)
    sns.countplot(data=data, x=feature)
    plt.title(f"Countplot of {feature}")
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Data Preparation
X = data.drop(['employment_type', 'job_title', 'salary_currency'], axis=1)
y = data['employment_type']

# Encoding categorical variables
label_encoders = {col: LabelEncoder().fit(X[col]) for col in X.select_dtypes(include=['object']).columns}
X = X.apply(lambda x: label_encoders[x.name].transform(x) if x.name in label_encoders else x)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression Classifier
clf_lr = LogisticRegression()
clf_lr.fit(X_train, y_train)

y_pred_lr = clf_lr.predict(X_test)


# Evaluation
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Classifier Accuracy:", accuracy_lr)
print("Classification Report for Logistic Regression Classifier:")
print(classification_report(y_test, y_pred_lr))


# Convert y_test and y_pred_lr to numerical labels for regression evaluation
le = LabelEncoder()
y_test_encoded = le.fit_transform(y_test)
y_pred_lr_encoded = le.transform(y_pred_lr)



plt.scatter(y_test,y_pred_lr);
plt.xlabel('Actual');
plt.ylabel('Predicted');

# Regression Evaluation Metrics
mse = mean_squared_error(y_test_encoded, y_pred_lr_encoded)
mae = mean_absolute_error(y_test_encoded, y_pred_lr_encoded)
r2 = r2_score(y_test_encoded, y_pred_lr_encoded)

# Printing the evaluation results
print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("R-squared (R2) Score:", r2)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Data Preparation
X = data.drop(['employment_type', 'job_title', 'salary_currency'], axis=1)
y = data['employment_type']

# Encoding categorical variables
label_encoders = {col: LabelEncoder().fit(X[col]) for col in X.select_dtypes(include=['object']).columns}
X = X.apply(lambda x: label_encoders[x.name].transform(x) if x.name in label_encoders else x)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Classifier
clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clf_rf.fit(X_train, y_train)
y_pred_rf = clf_rf.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Random Forest Classifier Accuracy:", accuracy_rf)
print("Classification Report for Random Forest Classifier:")
print(classification_report(y_test, y_pred_rf))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Data Preparation
X = data.drop(['employment_type', 'job_title', 'salary_currency'], axis=1)
y = data['employment_type']

# Encoding categorical variables
label_encoders = {col: LabelEncoder().fit(X[col]) for col in X.select_dtypes(include=['object']).columns}
X = X.apply(lambda x: label_encoders[x.name].transform(x) if x.name in label_encoders else x)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Gradient Boosting Classifier
clf_gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
clf_gb.fit(X_train, y_train)
y_pred_gb = clf_gb.predict(X_test)

# Evaluation
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print("Gradient Boosting Classifier Accuracy:", accuracy_gb)
print("Classification Report for Gradient Boosting Classifier:")
print(classification_report(y_test, y_pred_gb))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Data Preparation
X = data.drop(['employment_type', 'job_title', 'salary_currency'], axis=1)
y = data['employment_type']

# Encoding categorical variables
label_encoders = {col: LabelEncoder().fit(X[col]) for col in X.select_dtypes(include=['object']).columns}
X = X.apply(lambda x: label_encoders[x.name].transform(x) if x.name in label_encoders else x)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression Classifier
clf_lr = LogisticRegression()
clf_lr.fit(X_train, y_train)

y_pred_lr = clf_lr.predict(X_test)

# Classification Evaluation
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Classifier Accuracy:", accuracy_lr)
print("Classification Report for Logistic Regression Classifier:")
print(classification_report(y_test, y_pred_lr))

# Convert y_test and y_pred_lr to numerical labels for regression evaluation
le = LabelEncoder()
y_test_encoded = le.fit_transform(y_test)
y_pred_lr_encoded = le.transform(y_pred_lr)

# Regression Evaluation Metrics
mse = mean_squared_error(y_test_encoded, y_pred_lr_encoded)
mae = mean_absolute_error(y_test_encoded, y_pred_lr_encoded)
r2 = r2_score(y_test_encoded, y_pred_lr_encoded)

# Printing the evaluation results
print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("R-squared (R2) Score:", r2)